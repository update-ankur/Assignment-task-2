{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subtask_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nNqlideYCEl",
        "outputId": "dfcaee33-4226-4a2b-c7bc-c00eb964da65"
      },
      "source": [
        "\"\"\"\n",
        "getting zippped data file from URL\n",
        "\"\"\"\n",
        "\n",
        "!wget https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 09:28:53--  https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6032:18::a27d:5212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/pan6mutc5xj5kj0/trainPart1.zip [following]\n",
            "--2021-04-08 09:28:53--  https://www.dropbox.com/s/raw/pan6mutc5xj5kj0/trainPart1.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com/cd/0/inline/BMPffOpoGXVjJwtHO1ayuQsrSja7QlTKcDK6pb_lQ3wbUxZbGttqcWZ5Wq51QWgmGIYSeJpCpdkjv8vlSJCA7sLCigwaEwkwn4pIN7tuoq4BdF09oTqutiCFUOFWtKW7LFvL32OkcSEoA55TZEmhG7iS/file# [following]\n",
            "--2021-04-08 09:28:53--  https://uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com/cd/0/inline/BMPffOpoGXVjJwtHO1ayuQsrSja7QlTKcDK6pb_lQ3wbUxZbGttqcWZ5Wq51QWgmGIYSeJpCpdkjv8vlSJCA7sLCigwaEwkwn4pIN7tuoq4BdF09oTqutiCFUOFWtKW7LFvL32OkcSEoA55TZEmhG7iS/file\n",
            "Resolving uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com (uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com (uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BMNe5js8hW2TiCcrXODmNpMvLUWfwKaB2A_IhV7_yPVp_pHl-GVW9SX0809Jpzs_KDujxWNQ9c6JF-CbtiTTG0n99qbPnkBSHaKDec93Z0Nb_36OX5svrVBbMe8DXmh4HvAQ3aauUuFbTzWtaQC7OfSeeQbWO0ReLcgUONihb_iccfdTK6cqQwRlrstyKZFk4OOLsUPV2iW_5eDlRA8Gs5-_zgTA9z3XEa0h6FTKxd3ktuPxwfaNpmYnF8GGV_3RQDE2JihRZnAH7n6Fep9xj7mVEgtejxmKf256YYWuhnZMV9zs8n8GLCH9m5l__VjlXoZiDZnvS7cGO26jL5I_Ng_g7OVq0FNziplqfYqsSvUdnrrn47FP3V9wmrdIup_K-0w/file [following]\n",
            "--2021-04-08 09:28:54--  https://uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com/cd/0/inline2/BMNe5js8hW2TiCcrXODmNpMvLUWfwKaB2A_IhV7_yPVp_pHl-GVW9SX0809Jpzs_KDujxWNQ9c6JF-CbtiTTG0n99qbPnkBSHaKDec93Z0Nb_36OX5svrVBbMe8DXmh4HvAQ3aauUuFbTzWtaQC7OfSeeQbWO0ReLcgUONihb_iccfdTK6cqQwRlrstyKZFk4OOLsUPV2iW_5eDlRA8Gs5-_zgTA9z3XEa0h6FTKxd3ktuPxwfaNpmYnF8GGV_3RQDE2JihRZnAH7n6Fep9xj7mVEgtejxmKf256YYWuhnZMV9zs8n8GLCH9m5l__VjlXoZiDZnvS7cGO26jL5I_Ng_g7OVq0FNziplqfYqsSvUdnrrn47FP3V9wmrdIup_K-0w/file\n",
            "Reusing existing connection to uc13af2a0a296634cf59e3875d8a.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10059590 (9.6M) [application/zip]\n",
            "Saving to: ‘trainPart1.zip’\n",
            "\n",
            "trainPart1.zip      100%[===================>]   9.59M  9.56MB/s    in 1.0s    \n",
            "\n",
            "2021-04-08 09:28:56 (9.56 MB/s) - ‘trainPart1.zip’ saved [10059590/10059590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWryvLdBYSD4"
      },
      "source": [
        "!unzip trainPart1.zip\n",
        "\n",
        "\"\"\"\n",
        "Unzipping downloded file\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_GBsW_0YauR"
      },
      "source": [
        "\"\"\"\n",
        "chaning our working directory to \"train\",\n",
        "Next step will be to rename existing file of 0-9 digits and removing rest of the dataset(a-z,A-Z)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.chdir('/content/train')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbrllRFBYfLW"
      },
      "source": [
        "\"\"\"\n",
        "renaming files which contrains dataset for this subtask.\n",
        "(0-9 only)\n",
        "\"\"\"\n",
        "\n",
        "for i in range(1,11):\n",
        "  if i<10:\n",
        "    path='Sample00'+str(i)\n",
        "    os.rename(path, str(i-1))\n",
        "  if i==10:\n",
        "    path='Sample0'+str(i)\n",
        "    os.rename(path, str(i-1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIe2emptYhW4"
      },
      "source": [
        "\"\"\"\n",
        "this module helps you in copying and removal of files and directories \n",
        "\"\"\"\n",
        "import shutil"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH-59ZgiYjLW"
      },
      "source": [
        "for i in range(11,63):\n",
        "    path='Sample0'+str(i)\n",
        "    shutil.rmtree(path)  #removing directory at this path\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHnQzmJZYkjT"
      },
      "source": [
        "\"\"\"\n",
        "Impoting all neccessory modules for model building, compiling and training\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, sys\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGoXNncxYoLd"
      },
      "source": [
        "### Model Building\n",
        "\n",
        "In this case we have to train on images of 0-9 digits and which is kind of less classes than the last time.\n",
        "It's very obvious that we have small dataset but from last learnings i'm sure that i can build a descent model \n",
        "using this dataset\n",
        "\n",
        "\n",
        "This time the architecture is pretty much same as of the last time, in this model architecture creating a symmetry of last model(subtask 1)  was my first thought, secondary i thought to have more convolutional layers with more connections in exiting connections. \n",
        "\n",
        "So picking my first thought seems to be right choice as last time i got accurary above 90%.  \n",
        "Target is to achieve a descent accuracy this time as well(95%+ is descent. isn't?).   \n",
        "\n",
        "\n",
        "This architecture is inspired by a parallel and series resistance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BfeuS23Yl3j"
      },
      "source": [
        "def MiDasNet():\n",
        "  \"\"\"\n",
        "  As we have connections so working with Sequencial Model is not feasibile.\n",
        "  Hence in this model i'll be using keras Functional API(helps in creating flexible models)\n",
        "\n",
        "  \"\"\"\n",
        "  input=keras.Input(shape=(28,28,1)) #input shape this time is 28*28\n",
        "  input_2=layers.Conv2D(64,3,kernel_initializer='HeNormal',activation='relu',padding='same')(input)# passing input to this conv2d layer\n",
        "  input_2=layers.MaxPooling2D(2,2)(input_2) #applying max pooling\n",
        "  input_3=layers.Conv2D(64,3,padding='same',kernel_initializer='HeNormal',activation='relu')(input_2) #passing input_2 to this conv2d layer\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Now this input_3 will  be used by three layers layer_1,layer_2,layer_3 for leanring more complex features,\n",
        "  Keep in mind that we are using this with kernal_regularizer.\n",
        "  \"\"\"\n",
        "\n",
        "  layer_1=layers.Conv2D(32,1,padding='same',kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2')(input_3)#connection from input_3 layer to layer_1\n",
        "  layer_1=layers.BatchNormalization()(layer_1)\n",
        "  layer_1=layers.MaxPooling2D(2,2)(layer_1)\n",
        "  layer_1=layers.Dropout(0.3)(layer_1)\n",
        "\n",
        "  layer_2=layers.Conv2D(32,1,kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2',padding='same')(input_3)#connection from input_3 layer to layer_2\n",
        "  layer_2=layers.BatchNormalization()(layer_2)\n",
        "  layer_2=layers.MaxPooling2D((2,2))(layer_2)\n",
        "  layer_2=layers.Dropout(0.3)(layer_2)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Padding here is 'same' for keeping dimentions be equal at ending point of connection.\n",
        "  \"\"\"\n",
        "\n",
        "  layer_3=layers.Conv2D(32,1,kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2',padding='same')(input_3)#connection from input_3 layer to layer_3\n",
        "  layer_3=layers.BatchNormalization()(layer_3)\n",
        "  layer_3=layers.MaxPooling2D((2,2))(layer_3)\n",
        "  layer_3=layers.Dropout(0.3)(layer_3)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  As we have layer_1, layer_2, layer_3 with same output dimention, we can use Add for adding these tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  sum_out=layers.add([layer_1,layer_2,layer_3])\n",
        "    \n",
        "  \"\"\"\n",
        "  Passing this sum_out layer to a conv2d layers for learning more complex features\n",
        "  \"\"\"\n",
        "\n",
        "  input_2_2=layers.Conv2D(64,3,padding='same',kernel_initializer='HeNormal',activation='relu')(sum_out) \n",
        "  input_2_2=layers.MaxPooling2D(2,2)(input_2_2)\n",
        "  input_2_2=layers.Conv2D(64,3,padding='same',kernel_initializer='HeNormal',activation='relu')(input_2_2)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Now this input_2_2 will  be used by three layers layer_1_1,layer_2_2,layer_3_3 for leanring more complex features,\n",
        "  Keep in mind that we are using this with kernal_regularizer.\n",
        "  \"\"\"\n",
        "    \n",
        "    \n",
        "  layer_1_1=layers.Conv2D(32,1,padding='same',kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2')(input_2_2) # input from input_2_2 to layer_1_1\n",
        "  layer_1_1=layers.BatchNormalization()(layer_1_1)\n",
        "  layer_1_1=layers.MaxPooling2D(2,2)(layer_1_1)\n",
        "  layer_1_1=layers.Dropout(0.3)(layer_1_1)\n",
        "\n",
        "  layer_2_2=layers.Conv2D(32,1,padding='same',kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2')(input_2_2) # input from input_2_2 to layer_2_2\n",
        "  layer_2_2=layers.BatchNormalization()(layer_2_2)\n",
        "  layer_2_2=layers.MaxPooling2D((2,2))(layer_2_2)\n",
        "  layer_2_2=layers.Dropout(0.3)(layer_2_2)\n",
        "    \n",
        "  \"\"\"\n",
        "  Padding here is 'same' for keeping dimentions be equal at ending point of connection.\n",
        "  \"\"\"\n",
        "\n",
        "  layer_3_3=layers.Conv2D(32,1,padding='same',kernel_initializer='HeNormal',activation='relu',kernel_regularizer='l2')(input_2_2) # input from input_2_2 to layer_3_3\n",
        "  layer_3_3=layers.BatchNormalization()(layer_3_3)\n",
        "  layer_3_3=layers.MaxPooling2D((2,2))(layer_3_3)\n",
        "  layer_3_3=layers.Dropout(0.3)(layer_3_3)\n",
        "\n",
        "  \"\"\"\n",
        "  As we have layer_1_1, layer_2_2, layer_3_3 with same output dimention, we can use Add for adding these tensors.\n",
        "  \"\"\"\n",
        "  sum_out_2=layers.add([layer_1_1,layer_2_2,layer_3_3])\n",
        "\n",
        "\n",
        "  sum_out_2=layers.GlobalMaxPool2D()(sum_out_2)  #performing global maximum\n",
        "\n",
        "  sum_out_2=layers.Flatten()(sum_out_2)# flattens sum_out_2 tensor to 1d tensor \n",
        "  out=layers.Dense(10,'softmax')(sum_out_2) #output layer\n",
        "  model=keras.Model(input,out,name='MiDasNet')# grouping layer into object  https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOduoVxxYxY5"
      },
      "source": [
        "first_model=MiDasNet()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7a8mF2uY-U8",
        "outputId": "f794381e-92cc-46ed-8c17-5f520a3fc891"
      },
      "source": [
        "first_model.summary() # Summarize all the layers with corresponing #parameters to train."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"MiDasNet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 28, 28, 64)   640         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 14, 14, 64)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 14, 14, 64)   36928       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 14, 14, 32)   2080        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 32)   2080        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 14, 32)   2080        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 14, 14, 32)   128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 14, 14, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 14, 14, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 32)     0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 32)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 32)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 7, 7, 32)     0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 7, 7, 32)     0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 7, 7, 32)     0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 7, 7, 32)     0           dropout[0][0]                    \n",
            "                                                                 dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 7, 64)     18496       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 64)     0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 3, 3, 64)     36928       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 3, 3, 32)     2080        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 3, 3, 32)     2080        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 3, 3, 32)     2080        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 3, 3, 32)     128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 3, 3, 32)     128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 3, 3, 32)     128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 32)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 32)     0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 32)     0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1, 1, 32)     0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1, 1, 32)     0           max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 1, 1, 32)     0           max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 1, 1, 32)     0           dropout_3[0][0]                  \n",
            "                                                                 dropout_4[0][0]                  \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 32)           0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 32)           0           global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           330         flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 106,570\n",
            "Trainable params: 106,186\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNPJRFuiZBnf"
      },
      "source": [
        "\"\"\"\n",
        "Compiling model.\n",
        "\"\"\"\n",
        "first_model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-9UXWvXZF_g",
        "outputId": "b38e2ddd-0df9-439f-9357-e94633453ec1"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   data_format='channels_last')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ImageDataGenerator is used as Image data augmentation in real-time but here it is used for rescaling and for loading data.\n",
        "flow_from_directory is used which helps in resizing image and creating batch of themand \n",
        "that will be later passed to model as a input.\n",
        "flow_from_directory automatically infers class label with the folder name. \n",
        "\"\"\"\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/train',\n",
        "                                                 target_size = (28, 28),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 color_mode='grayscale',\n",
        "                                                 subset=\"training\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF49yam8ZXc0",
        "outputId": "96df5783-297b-4ab6-ba20-f9f08890523a"
      },
      "source": [
        "training_set.image_shape\n",
        "\n",
        "#output from this generator will have shape of (batch_size,training_set.image_shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1bEG4tuZcA2",
        "outputId": "de759290-1daa-4159-c812-2d5ce36d5a63"
      },
      "source": [
        "history=first_model.fit(training_set,epochs=100,verbose=1) # train model for 100 epochs"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 36s 457ms/step - loss: 8.2951 - accuracy: 0.1507\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 6s 467ms/step - loss: 6.6699 - accuracy: 0.1257\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 6.2214 - accuracy: 0.1954\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 5.6581 - accuracy: 0.2481\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 6s 491ms/step - loss: 5.4041 - accuracy: 0.2756\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 5.1436 - accuracy: 0.3111\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 4.8696 - accuracy: 0.4422\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 4.6523 - accuracy: 0.4148\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 4.4616 - accuracy: 0.4873\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 4.2148 - accuracy: 0.5301\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 3.9586 - accuracy: 0.5594\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 3.7912 - accuracy: 0.6282\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 3.7107 - accuracy: 0.6085\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 3.5970 - accuracy: 0.5969\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 3.4289 - accuracy: 0.6524\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 3.1810 - accuracy: 0.6876\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 3.1554 - accuracy: 0.6964\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 2.9383 - accuracy: 0.7480\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 2.8041 - accuracy: 0.7804\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 2.7604 - accuracy: 0.7409\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 2.6306 - accuracy: 0.7882\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 2.6147 - accuracy: 0.7742\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 2.4853 - accuracy: 0.7817\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 2.3445 - accuracy: 0.8441\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 6s 466ms/step - loss: 2.1844 - accuracy: 0.8580\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 2.2554 - accuracy: 0.8100\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 2.0190 - accuracy: 0.8941\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.9969 - accuracy: 0.9109\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.8998 - accuracy: 0.9253\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 2.1182 - accuracy: 0.8192\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.9068 - accuracy: 0.8789\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 1.9254 - accuracy: 0.8950\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 1.8208 - accuracy: 0.8947\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 1.7483 - accuracy: 0.9182\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 1.6516 - accuracy: 0.9279\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.5848 - accuracy: 0.9461\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 1.5215 - accuracy: 0.9621\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.5014 - accuracy: 0.9648\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 1.4816 - accuracy: 0.9474\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.4742 - accuracy: 0.9480\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.4710 - accuracy: 0.9260\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 6s 493ms/step - loss: 1.3907 - accuracy: 0.9500\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 1.3571 - accuracy: 0.9598\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 1.3668 - accuracy: 0.9371\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.2961 - accuracy: 0.9567\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.2481 - accuracy: 0.9591\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 6s 477ms/step - loss: 1.2039 - accuracy: 0.9726\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.2051 - accuracy: 0.9674\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.1789 - accuracy: 0.9663\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 1.1394 - accuracy: 0.9832\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.1315 - accuracy: 0.9646\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 1.0711 - accuracy: 0.9794\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.0682 - accuracy: 0.9774\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 1.0552 - accuracy: 0.9729\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 1.0450 - accuracy: 0.9644\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 1.0029 - accuracy: 0.9806\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 0.9924 - accuracy: 0.9763\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.9689 - accuracy: 0.9860\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 6s 491ms/step - loss: 0.9146 - accuracy: 0.9966\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 0.9047 - accuracy: 0.9940\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.9294 - accuracy: 0.9725\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 0.8834 - accuracy: 0.9944\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 0.8494 - accuracy: 0.9933\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 0.8363 - accuracy: 0.9971\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.8333 - accuracy: 0.9803\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.8452 - accuracy: 0.9764\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.8433 - accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.8032 - accuracy: 0.9805\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 0.8646 - accuracy: 0.9725\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 0.7929 - accuracy: 0.9688\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 6s 479ms/step - loss: 0.7731 - accuracy: 0.9748\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 0.7478 - accuracy: 0.9838\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 0.7479 - accuracy: 0.9707\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.7256 - accuracy: 0.9901\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 6s 469ms/step - loss: 0.7085 - accuracy: 0.9856\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6965 - accuracy: 0.9812\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 0.6773 - accuracy: 0.9930\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6633 - accuracy: 0.9879\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.6364 - accuracy: 0.9933\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 0.6813 - accuracy: 0.9853\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6632 - accuracy: 0.9794\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6436 - accuracy: 0.9808\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6204 - accuracy: 0.9894\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 6s 468ms/step - loss: 0.6139 - accuracy: 0.9853\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6032 - accuracy: 0.9884\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 0.5947 - accuracy: 0.9849\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 6s 492ms/step - loss: 0.6178 - accuracy: 0.9781\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 0.6020 - accuracy: 0.9861\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 6s 474ms/step - loss: 0.5647 - accuracy: 0.9881\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6132 - accuracy: 0.9575\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 6s 473ms/step - loss: 0.5734 - accuracy: 0.9845\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.6114 - accuracy: 0.9515\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.5357 - accuracy: 0.9916\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 0.5818 - accuracy: 0.9750\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 6s 476ms/step - loss: 0.5241 - accuracy: 0.9883\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 6s 479ms/step - loss: 0.5221 - accuracy: 0.9843\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 6s 471ms/step - loss: 0.5022 - accuracy: 0.9919\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 6s 467ms/step - loss: 0.5162 - accuracy: 0.9849\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 6s 472ms/step - loss: 0.5343 - accuracy: 0.9767\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 6s 475ms/step - loss: 0.4972 - accuracy: 0.9827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_uIZ30AlaMZ"
      },
      "source": [
        "*Still there are chances(possibility) that loss might reduce <.5 but keeping   \n",
        "in mind that we have less data and model can overfit very easily, will see that   \n",
        "when we test this model on mnist dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9LwmLKLlZ9j"
      },
      "source": [
        "## Loading MNIST DATA \n",
        "\n",
        "\n",
        "Loading this data is very easy as keras provides this dataset.  \n",
        "Data is in standard train test split i.e.. training set of 60,000 and a test set of 10,000 image dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xChbVY9OZjH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44f9188-ef25-45ad-f0dc-2dee73731be4"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test)=keras.datasets.mnist.load_data()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayK0YCaNgF2S"
      },
      "source": [
        "\"\"\"\n",
        "y_test is 1-D array having values from 0-9., \n",
        "Performing one-hot encoding on them.\n",
        "\"\"\"\n",
        "\n",
        "y_train=tf.one_hot(y_train,depth=10)\n",
        "y_test=tf.one_hot(y_test,depth=10)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-V2wZcmBTj"
      },
      "source": [
        "##### Using pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G19ieDN0gGn7",
        "outputId": "0b410a35-9862-4015-de17-cf5b3255c6d0"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Training on pretrained model using MNIST DataSet\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "history2=first_model.fit(x=x_train,y=y_train,batch_size=128,epochs=100,validation_split=0.2,use_multiprocessing=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 11s 26ms/step - loss: 0.6786 - accuracy: 0.9147 - val_loss: 0.4070 - val_accuracy: 0.9710\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.3536 - accuracy: 0.9688 - val_loss: 0.2900 - val_accuracy: 0.9728\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.2397 - accuracy: 0.9764 - val_loss: 0.1827 - val_accuracy: 0.9849\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.1730 - accuracy: 0.9795 - val_loss: 0.1566 - val_accuracy: 0.9793\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.1345 - accuracy: 0.9820 - val_loss: 0.1132 - val_accuracy: 0.9851\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.1091 - accuracy: 0.9837 - val_loss: 0.0965 - val_accuracy: 0.9846\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0895 - accuracy: 0.9859 - val_loss: 0.1034 - val_accuracy: 0.9808\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0767 - accuracy: 0.9869 - val_loss: 0.0694 - val_accuracy: 0.9893\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0703 - accuracy: 0.9879 - val_loss: 0.0844 - val_accuracy: 0.9833\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0645 - accuracy: 0.9884 - val_loss: 0.0964 - val_accuracy: 0.9808\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0616 - accuracy: 0.9885 - val_loss: 0.0681 - val_accuracy: 0.9864\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0557 - accuracy: 0.9897 - val_loss: 0.0480 - val_accuracy: 0.9919\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0531 - accuracy: 0.9903 - val_loss: 0.0614 - val_accuracy: 0.9878\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0509 - accuracy: 0.9903 - val_loss: 0.0606 - val_accuracy: 0.9879\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0477 - accuracy: 0.9914 - val_loss: 0.0617 - val_accuracy: 0.9885\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0479 - accuracy: 0.9915 - val_loss: 0.0704 - val_accuracy: 0.9856\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0483 - accuracy: 0.9911 - val_loss: 0.0580 - val_accuracy: 0.9893\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0395 - accuracy: 0.9933 - val_loss: 0.0451 - val_accuracy: 0.9917\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0388 - accuracy: 0.9931 - val_loss: 0.0600 - val_accuracy: 0.9902\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0408 - accuracy: 0.9930 - val_loss: 0.0548 - val_accuracy: 0.9897\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0350 - accuracy: 0.9934 - val_loss: 0.0747 - val_accuracy: 0.9835\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 9s 25ms/step - loss: 0.0370 - accuracy: 0.9937 - val_loss: 0.0499 - val_accuracy: 0.9906\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0349 - accuracy: 0.9936 - val_loss: 0.0434 - val_accuracy: 0.9925\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0340 - accuracy: 0.9940 - val_loss: 0.0455 - val_accuracy: 0.9906\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0305 - accuracy: 0.9950 - val_loss: 0.0541 - val_accuracy: 0.9891\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0364 - accuracy: 0.9935 - val_loss: 0.0545 - val_accuracy: 0.9913\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0332 - accuracy: 0.9942 - val_loss: 0.0544 - val_accuracy: 0.9897\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0318 - accuracy: 0.9950 - val_loss: 0.0458 - val_accuracy: 0.9920\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0296 - accuracy: 0.9954 - val_loss: 0.0636 - val_accuracy: 0.9879\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0297 - accuracy: 0.9952 - val_loss: 0.0491 - val_accuracy: 0.9915\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0307 - accuracy: 0.9950 - val_loss: 0.0652 - val_accuracy: 0.9869\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0293 - accuracy: 0.9956 - val_loss: 0.0438 - val_accuracy: 0.9923\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0276 - accuracy: 0.9954 - val_loss: 0.0499 - val_accuracy: 0.9912\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0274 - accuracy: 0.9954 - val_loss: 0.0561 - val_accuracy: 0.9907\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0248 - accuracy: 0.9962 - val_loss: 0.0666 - val_accuracy: 0.9893\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0263 - accuracy: 0.9960 - val_loss: 0.0508 - val_accuracy: 0.9896\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0261 - accuracy: 0.9960 - val_loss: 0.0443 - val_accuracy: 0.9923\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0245 - accuracy: 0.9964 - val_loss: 0.0510 - val_accuracy: 0.9918\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0255 - accuracy: 0.9959 - val_loss: 0.0705 - val_accuracy: 0.9865\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0230 - accuracy: 0.9968 - val_loss: 0.0536 - val_accuracy: 0.9909\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0235 - accuracy: 0.9968 - val_loss: 0.0445 - val_accuracy: 0.9923\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0257 - accuracy: 0.9960 - val_loss: 0.0450 - val_accuracy: 0.9937\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0222 - accuracy: 0.9970 - val_loss: 0.0400 - val_accuracy: 0.9936\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0241 - accuracy: 0.9966 - val_loss: 0.0607 - val_accuracy: 0.9891\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0224 - accuracy: 0.9969 - val_loss: 0.0520 - val_accuracy: 0.9917\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0241 - accuracy: 0.9964 - val_loss: 0.0632 - val_accuracy: 0.9889\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0210 - accuracy: 0.9974 - val_loss: 0.0602 - val_accuracy: 0.9907\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0227 - accuracy: 0.9971 - val_loss: 0.0488 - val_accuracy: 0.9904\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0191 - accuracy: 0.9978 - val_loss: 0.0557 - val_accuracy: 0.9898\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0209 - accuracy: 0.9973 - val_loss: 0.0458 - val_accuracy: 0.9923\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0225 - accuracy: 0.9970 - val_loss: 0.0515 - val_accuracy: 0.9921\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0214 - accuracy: 0.9974 - val_loss: 0.0617 - val_accuracy: 0.9902\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0206 - accuracy: 0.9974 - val_loss: 0.0529 - val_accuracy: 0.9908\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0180 - accuracy: 0.9978 - val_loss: 0.0534 - val_accuracy: 0.9904\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 0.0574 - val_accuracy: 0.9904\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0182 - accuracy: 0.9978 - val_loss: 0.0494 - val_accuracy: 0.9927\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0217 - accuracy: 0.9971 - val_loss: 0.0473 - val_accuracy: 0.9917\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0177 - accuracy: 0.9980 - val_loss: 0.0481 - val_accuracy: 0.9920\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0213 - accuracy: 0.9974 - val_loss: 0.0586 - val_accuracy: 0.9912\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0207 - accuracy: 0.9974 - val_loss: 0.0635 - val_accuracy: 0.9893\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0182 - accuracy: 0.9980 - val_loss: 0.0559 - val_accuracy: 0.9912\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0207 - accuracy: 0.9971 - val_loss: 0.0501 - val_accuracy: 0.9927\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0203 - accuracy: 0.9975 - val_loss: 0.0509 - val_accuracy: 0.9918\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0166 - accuracy: 0.9979 - val_loss: 0.0577 - val_accuracy: 0.9912\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0179 - accuracy: 0.9979 - val_loss: 0.0576 - val_accuracy: 0.9904\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0173 - accuracy: 0.9979 - val_loss: 0.0596 - val_accuracy: 0.9900\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0181 - accuracy: 0.9982 - val_loss: 0.0581 - val_accuracy: 0.9915\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0217 - accuracy: 0.9976 - val_loss: 0.0547 - val_accuracy: 0.9918\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0173 - accuracy: 0.9982 - val_loss: 0.0494 - val_accuracy: 0.9907\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0204 - accuracy: 0.9974 - val_loss: 0.0626 - val_accuracy: 0.9906\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0172 - accuracy: 0.9981 - val_loss: 0.0533 - val_accuracy: 0.9904\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0188 - accuracy: 0.9973 - val_loss: 0.0533 - val_accuracy: 0.9905\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0174 - accuracy: 0.9979 - val_loss: 0.0567 - val_accuracy: 0.9918\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0144 - accuracy: 0.9984 - val_loss: 0.0488 - val_accuracy: 0.9905\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0182 - accuracy: 0.9975 - val_loss: 0.0688 - val_accuracy: 0.9893\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0147 - accuracy: 0.9988 - val_loss: 0.0465 - val_accuracy: 0.9920\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0192 - accuracy: 0.9977 - val_loss: 0.0590 - val_accuracy: 0.9908\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0168 - accuracy: 0.9980 - val_loss: 0.0634 - val_accuracy: 0.9895\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0176 - accuracy: 0.9981 - val_loss: 0.0493 - val_accuracy: 0.9919\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0183 - accuracy: 0.9980 - val_loss: 0.0601 - val_accuracy: 0.9905\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0173 - accuracy: 0.9983 - val_loss: 0.0519 - val_accuracy: 0.9919\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0175 - accuracy: 0.9982 - val_loss: 0.0439 - val_accuracy: 0.9927\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0174 - accuracy: 0.9979 - val_loss: 0.0572 - val_accuracy: 0.9912\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0162 - accuracy: 0.9983 - val_loss: 0.0528 - val_accuracy: 0.9909\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0147 - accuracy: 0.9988 - val_loss: 0.0613 - val_accuracy: 0.9906\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0143 - accuracy: 0.9985 - val_loss: 0.0888 - val_accuracy: 0.9869\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0181 - accuracy: 0.9979 - val_loss: 0.0688 - val_accuracy: 0.9887\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0193 - accuracy: 0.9980 - val_loss: 0.0542 - val_accuracy: 0.9917\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0141 - accuracy: 0.9987 - val_loss: 0.0527 - val_accuracy: 0.9914\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0181 - accuracy: 0.9977 - val_loss: 0.0633 - val_accuracy: 0.9908\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0137 - accuracy: 0.9991 - val_loss: 0.0495 - val_accuracy: 0.9911\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0160 - accuracy: 0.9981 - val_loss: 0.0899 - val_accuracy: 0.9880\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0187 - accuracy: 0.9981 - val_loss: 0.0502 - val_accuracy: 0.9917\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0152 - accuracy: 0.9984 - val_loss: 0.0521 - val_accuracy: 0.9919\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0165 - accuracy: 0.9984 - val_loss: 0.0603 - val_accuracy: 0.9911\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.0597 - val_accuracy: 0.9906\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0154 - accuracy: 0.9983 - val_loss: 0.0628 - val_accuracy: 0.9893\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.0508 - val_accuracy: 0.9918\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0167 - accuracy: 0.9982 - val_loss: 0.0452 - val_accuracy: 0.9930\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0161 - accuracy: 0.9985 - val_loss: 0.0589 - val_accuracy: 0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKNdlAqhmgZ7"
      },
      "source": [
        "##### Using Randomly Initialized Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzOfLf4Cmcw8"
      },
      "source": [
        "def SimpleModel():\n",
        "\n",
        "    model= keras.Sequential() #it's sequential model\n",
        "    model.add(layers.Conv2D(64,3,input_shape=(28,28,1),activation='relu',kernel_initializer='HeUniform')) #adding convolution layer \n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2),data_format=\"channels_last\")) # maxpooing the previous layer\n",
        "    model.add(layers.Dropout(rate=0.3)) #helps as a regularizer(weak)\n",
        "\n",
        "    model.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu',kernel_initializer='HeUniform'))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2),data_format=\"channels_last\"))\n",
        "    model.add(layers.Dropout(rate=0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu',kernel_initializer='HeUniform'))\n",
        "    model.add(layers.MaxPool2D(pool_size=(2,2),data_format=\"channels_last\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dropout(rate=0.3))\n",
        "\n",
        "\n",
        "    model.add(layers.Flatten())  # flattens everything form last layer\n",
        "\n",
        "    model.add(layers.Dense(16,activation='relu',kernel_initializer='HeUniform')) #adding dense layer\n",
        "    model.add(layers.BatchNormalization()) # Performing BatchNormalization on that\n",
        "    model.add(layers.Dropout(rate=0.3))\n",
        "\n",
        "    model.add(layers.Dense(10,activation='softmax')) #Output layer\n",
        "\n",
        "    return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOFi9QJhugbs",
        "outputId": "c347083a-a103-4676-eb73-be8d4a02a719"
      },
      "source": [
        "second_model=SimpleModel()\n",
        "second_model.summary() # Summarize all the layers with corresponing parameters to learn."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1, 1, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1, 1, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 29,242\n",
            "Trainable params: 29,146\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_j6aw_8upYN"
      },
      "source": [
        "\"\"\"\n",
        "Compiling model.\n",
        "\"\"\"\n",
        "\n",
        "second_model.compile(tf.keras.optimizers.Adam(learning_rate=0.009), #using adam optimizer\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(), # we have multiclass-classification problem\n",
        "              metrics=['accuracy']) #measures models performance\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sTBsRLOwrI1"
      },
      "source": [
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "\n",
        "# y_train=tf.one_hot(y_train,depth=10)\n",
        "# y_test=tf.one_hot(y_test,depth=10)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J_2unzXvBrb",
        "outputId": "ef875a09-7cb8-4ff6-d624-ab9dc1e20712"
      },
      "source": [
        "his=second_model.fit(x=x_train,y=y_train,batch_size=128,epochs=100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 1.0512 - accuracy: 0.6518\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2987 - accuracy: 0.9103\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2422 - accuracy: 0.9296\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2098 - accuracy: 0.9385\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1970 - accuracy: 0.9419\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1838 - accuracy: 0.9463\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1764 - accuracy: 0.9489\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1707 - accuracy: 0.9517\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1630 - accuracy: 0.9531\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1680 - accuracy: 0.9528\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1577 - accuracy: 0.9557\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1539 - accuracy: 0.9568\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1477 - accuracy: 0.9570\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1461 - accuracy: 0.9589\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1488 - accuracy: 0.9573\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1462 - accuracy: 0.9590\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1439 - accuracy: 0.9583\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1434 - accuracy: 0.9584\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1441 - accuracy: 0.9596\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1400 - accuracy: 0.9590\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1358 - accuracy: 0.9609\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1313 - accuracy: 0.9615\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1338 - accuracy: 0.9627\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1301 - accuracy: 0.9619\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1307 - accuracy: 0.9631\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1274 - accuracy: 0.9636\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1273 - accuracy: 0.9642\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1232 - accuracy: 0.9643\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1288 - accuracy: 0.9641\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1235 - accuracy: 0.9637\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1207 - accuracy: 0.9651\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1158 - accuracy: 0.9670\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1220 - accuracy: 0.9652\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1190 - accuracy: 0.9655\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1178 - accuracy: 0.9650\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1156 - accuracy: 0.9684\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1189 - accuracy: 0.9664\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1145 - accuracy: 0.9673\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1169 - accuracy: 0.9660\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1179 - accuracy: 0.9664\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1154 - accuracy: 0.9673\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1132 - accuracy: 0.9673\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1210 - accuracy: 0.9649\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1114 - accuracy: 0.9685\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1184 - accuracy: 0.9666\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1089 - accuracy: 0.9684\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1045 - accuracy: 0.9700\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1075 - accuracy: 0.9687\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1141 - accuracy: 0.9664\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1121 - accuracy: 0.9674\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1083 - accuracy: 0.9687\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1128 - accuracy: 0.9678\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1089 - accuracy: 0.9692\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1125 - accuracy: 0.9663\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1121 - accuracy: 0.9683\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1066 - accuracy: 0.9696\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1067 - accuracy: 0.9687\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1040 - accuracy: 0.9703\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1057 - accuracy: 0.9698\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1018 - accuracy: 0.9708\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1044 - accuracy: 0.9693\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1056 - accuracy: 0.9698\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1026 - accuracy: 0.9703\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1094 - accuracy: 0.9688\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1033 - accuracy: 0.9708\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1055 - accuracy: 0.9698\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1074 - accuracy: 0.9698\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1089 - accuracy: 0.9680\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1034 - accuracy: 0.9706\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1037 - accuracy: 0.9702\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1003 - accuracy: 0.9722\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1036 - accuracy: 0.9694\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0994 - accuracy: 0.9714\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1019 - accuracy: 0.9709\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1014 - accuracy: 0.9702\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1037 - accuracy: 0.9700\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1019 - accuracy: 0.9705\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1005 - accuracy: 0.9698\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1041 - accuracy: 0.9704\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0995 - accuracy: 0.9707\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1009 - accuracy: 0.9709\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1007 - accuracy: 0.9714\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0982 - accuracy: 0.9708\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1033 - accuracy: 0.9713\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0991 - accuracy: 0.9713\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0957 - accuracy: 0.9728\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1054 - accuracy: 0.9694\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1003 - accuracy: 0.9713\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1004 - accuracy: 0.9714\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0984 - accuracy: 0.9720\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0969 - accuracy: 0.9711\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0976 - accuracy: 0.9723\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0974 - accuracy: 0.9731\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0979 - accuracy: 0.9712\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0958 - accuracy: 0.9729\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0969 - accuracy: 0.9720\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0971 - accuracy: 0.9721\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0958 - accuracy: 0.9726\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0906 - accuracy: 0.9734\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0970 - accuracy: 0.9722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "W-FoIGIMvfgh",
        "outputId": "6e6493c5-2fdf-4a53-e41f-0c5fca5f72a9"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N54iSw2vkKrI"
      },
      "source": [
        "#### Convergence Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "kcWZ3_zQlT4e",
        "outputId": "80c6904d-55f9-4143-80d9-23c009ad5884"
      },
      "source": [
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(his.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['first_model', 'second_model'], loc='bottom left')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: Unrecognized location 'bottom left'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-SbvoMmTR5"
      },
      "source": [
        "In can be seen that first_model converges faster than than the second_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4kqbsalUgp"
      },
      "source": [
        "#### Comparing model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BUNEI0ZyGct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "4eeb6c96-ae39-4a30-d8dd-13da5eab06d4"
      },
      "source": [
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(his.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['first_model', 'second_model'], loc='bottom left')\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: Unrecognized location 'bottom left'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjmYHH9bmeRa"
      },
      "source": [
        "In can be seen that first_model has good performance in comparison to second_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsp0erKanErj",
        "outputId": "f195fd01-246f-442d-e509-650461db3a74"
      },
      "source": [
        "#Let's check it's performance on MNIST test DATA \n",
        "first_model.evaluate(x=x_test,y=y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0556 - accuracy: 0.9910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05557715520262718, 0.9909999966621399]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvAlw5TCnMjk",
        "outputId": "bf9c3143-95cd-405a-82a7-ec8584af375d"
      },
      "source": [
        "second_model.evaluate(x=x_test,y=y_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0370 - accuracy: 0.9892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03698917478322983, 0.9891999959945679]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC171pD2ndpm"
      },
      "source": [
        "Performance on test data is descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAUsEE0yQZht"
      },
      "source": [
        "***Saving Models***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZTAGrAoI4-t"
      },
      "source": [
        "second_model.save('second_modeltask2.h5') # saving weights\n",
        "\n",
        "Modell = second_model.to_json()\n",
        "#save the model architecture to JSON file\n",
        "with open('second_modeltask2.json', 'w') as json_file:\n",
        "    json_file.write(Modell)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaO00ozyLJsJ"
      },
      "source": [
        "first_model.save('first_modeltask2.h5') # saving weights\n",
        "\n",
        "Modell = first_model.to_json()\n",
        "#save the model architecture to JSON file\n",
        "with open('first_modeltask2.json', 'w') as json_file:\n",
        "    json_file.write(Modell)"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}